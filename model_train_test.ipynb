{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ML_654 Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kjc9Hfpd8JJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cbc1c26-9bda-427e-f3dd-e34dd4e64bff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/Shareddrives/Machine Learning - Transformers\""
      ],
      "metadata": {
        "id": "Om6Y18da98DT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaafdec2-c352-4789-b697-b3d8b12bda4a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/Machine Learning - Transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "tLUK5noKQWZf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "oQCzsoRMPnWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify_data(X_local, X_remote, y, batch_size = 24):\n",
        "    batches = []\n",
        "    for i in range(y.shape[0]//batch_size):\n",
        "        batches.append([X_local[i*batch_size:(i+1)*batch_size], X_remote[i*batch_size:(i+1)*batch_size], y[i*batch_size:(i+1)*batch_size]])\n",
        "    return batches"
      ],
      "metadata": {
        "id": "gqyR4zQMPnKy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model"
      ],
      "metadata": {
        "id": "CGaAGpJL-GUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "Fq11QuUh-Ti5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_model, dropout_p, max_len):\n",
        "        super().__init__()\n",
        "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "        # max_len determines how far the position can have an effect on a token (window)\n",
        "        \n",
        "        # Info\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        \n",
        "        # Encoding - From formula\n",
        "        pos_encoding = torch.zeros(max_len, dim_model)\n",
        "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
        "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
        "        \n",
        "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
        "        \n",
        "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
        "        \n",
        "        # Saving buffer (same as parameter without gradients needed)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
        "        \n",
        "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
        "        # Residual connection + pos encoding\n",
        "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
      ],
      "metadata": {
        "id": "JkHsbKq4BK3e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    # Constructor\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_tokens,\n",
        "        dim_model,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dropout_p,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # INFO\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.dim_model = dim_model\n",
        "\n",
        "        # LAYERS\n",
        "        self.positional_encoder = PositionalEncoding(\n",
        "            dim_model=dim_model, dropout_p=dropout_p, max_len=50000\n",
        "        )\n",
        "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=dim_model,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dropout=dropout_p,\n",
        "        )\n",
        "        # self.out = nn.Linear(dim_model, num_tokens)\n",
        "        \n",
        "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
        "        # Src size must be (batch_size, src sequence length)\n",
        "        # Tgt size must be (batch_size, tgt sequence length)\n",
        "\n",
        "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
        "        src = src.long()\n",
        "        tgt = tgt.long()\n",
        "        # print(f\"src size {src.shape} \\t tgt size {tgt.shape}\")\n",
        "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
        "        src = self.positional_encoder(src)\n",
        "        tgt = self.positional_encoder(tgt)\n",
        "        \n",
        "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
        "        # to obtain size (sequence length, batch_size, dim_model),\n",
        "        src = src.permute(1,0,2)\n",
        "        tgt = tgt.permute(1,0,2)\n",
        "\n",
        "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
        "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
        "        # out = self.out(transformer_out)\n",
        "        out = transformer_out\n",
        "        \n",
        "        return out\n",
        "      \n",
        "    def get_tgt_mask(self, size) -> torch.tensor:\n",
        "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
        "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
        "        mask = mask.float()\n",
        "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
        "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
        "        \n",
        "        # EX for size=5:\n",
        "        # [[0., -inf, -inf, -inf, -inf],\n",
        "        #  [0.,   0., -inf, -inf, -inf],\n",
        "        #  [0.,   0.,   0., -inf, -inf],\n",
        "        #  [0.,   0.,   0.,   0., -inf],\n",
        "        #  [0.,   0.,   0.,   0.,   0.]]\n",
        "        \n",
        "        return mask\n",
        "    \n",
        "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
        "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
        "        # [False, False, False, True, True, True]\n",
        "        return (matrix == pad_token)"
      ],
      "metadata": {
        "id": "QpLIfvP1-e7Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Stations model"
      ],
      "metadata": {
        "id": "b49qcVmmZQQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LocalStation(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_model,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dropout_p,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_type = 'local stations model'\n",
        "\n",
        "        # LAYERS\n",
        "        self.linear1 = nn.Linear(dim_model[1], 300)\n",
        "        self.linear2 = nn.Linear(300, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.transformer = Transformer(num_tokens=23, dim_model=dim_model[0], num_heads=num_heads, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dropout_p=dropout_p)\n",
        "        self.out = nn.Linear(23, 1)\n",
        "    \n",
        "\n",
        "    def forward(self, X):\n",
        "        linear1_out = self.linear1(X)\n",
        "        linear2_out = self.linear2(linear1_out)\n",
        "        linear2_out = self.relu(linear2_out)\n",
        "        linear2_out_reshaped = torch.reshape(linear2_out, (linear2_out.shape[0], -1))\n",
        "\n",
        "        sequence_length = linear2_out_reshaped[:,:-1].size(1)\n",
        "\n",
        "        tgt_mask = self.transformer.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "        transformer_out = self.transformer(linear2_out_reshaped[:,:-1], linear2_out_reshaped[:,1:], tgt_mask)\n",
        "        transformer_out = transformer_out.permute(1,2,0)\n",
        "        \n",
        "        out = self.out(transformer_out)\n",
        "        out = torch.reshape(out, (out.shape[0], -1))\n",
        "        return out"
      ],
      "metadata": {
        "id": "7tSX_UPh2a00"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remote Stations Model"
      ],
      "metadata": {
        "id": "3CosH8TtPyEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RemoteStation(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_model,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dropout_p,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_type = 'remote stations model'\n",
        "\n",
        "        # LAYERS\n",
        "        self.linear1 = nn.Linear(dim_model[2], 1)\n",
        "        self.linear2 = nn.Linear(dim_model[0], 300)\n",
        "        self.linear3 = nn.Linear(300,1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.transformer = Transformer(num_tokens=23, dim_model= dim_model[1], num_heads=num_heads, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dropout_p=dropout_p)\n",
        "        self.out = nn.Linear(23, 1)\n",
        "    \n",
        "\n",
        "    def forward(self, X):\n",
        "        linear1_out = self.linear1(X)\n",
        "        linear1_out = torch.reshape(linear1_out, (linear1_out.shape[0], linear1_out.shape[1], -1))\n",
        "        linear1_out = linear1_out.permute(0,2,1)\n",
        "        linear2_out = self.linear2(linear1_out)\n",
        "        linear3_out = self.linear3(linear2_out)\n",
        "        linear3_out = self.relu(linear3_out)\n",
        "        linear3_out = linear3_out.permute(0,2,1)\n",
        "        linear3_out_reshaped = torch.reshape(linear3_out, (linear3_out.shape[0], -1))\n",
        "\n",
        "\n",
        "        sequence_length = linear3_out_reshaped[:,:-1].size(1)\n",
        "\n",
        "        tgt_mask = self.transformer.get_tgt_mask(sequence_length).to(device)\n",
        "\n",
        "        transformer_out = self.transformer(linear3_out_reshaped[:,:-1], linear3_out_reshaped[:,1:], tgt_mask)\n",
        "        transformer_out = transformer_out.permute(1,2,0)\n",
        "        \n",
        "        out = self.out(transformer_out)\n",
        "        out = torch.reshape(out, (out.shape[0], -1))\n",
        "        return out"
      ],
      "metadata": {
        "id": "3-li1Iz1C7Vf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining Both and Final Model"
      ],
      "metadata": {
        "id": "z0L-DS_SQGx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_model_local,\n",
        "        dim_model_remote,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dropout_p,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.local = LocalStation(\n",
        "            dim_model=dim_model_local, num_heads=num_heads, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dropout_p=dropout_p\n",
        "        )  \n",
        "        self.linear1 = nn.Linear(dim_model_local[0], 300)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.remote = RemoteStation(\n",
        "            dim_model=dim_model_remote, num_heads=num_heads, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dropout_p=dropout_p\n",
        "        )\n",
        "        self.linear2 = nn.Linear(dim_model_remote[1], 300)\n",
        "\n",
        "        self.linear3 = nn.Linear(300, 1)\n",
        "        self.out = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, X_local, X_remote):\n",
        "        local_out = self.local(X_local)\n",
        "        local_linear_out = self.linear1(local_out)\n",
        "        local_linear_out = self.relu(local_linear_out)\n",
        "        local_linear_out = torch.reshape(local_linear_out, (local_linear_out.shape[0], 1, -1))\n",
        "\n",
        "        remote_out = self.remote(X_remote)\n",
        "        remote_linear_out = self.linear2(remote_out)\n",
        "        remote_linear_out = self.relu(remote_linear_out)\n",
        "        remote_linear_out = torch.reshape(remote_linear_out, (remote_linear_out.shape[0], 1, -1))\n",
        "\n",
        "        concate = torch.cat([local_linear_out, remote_linear_out], dim = 1)\n",
        "\n",
        "        linear3_out = self.linear3(concate)\n",
        "        linear3_out = self.relu(linear3_out)\n",
        "        linear3_out = torch.reshape(linear3_out, (linear3_out.shape[0], -1))\n",
        "        out = self.out(linear3_out)\n",
        "        out = self.relu(out)\n",
        "        out = torch.reshape(out, (out.shape[0],))\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "q-9L-l2iFnB0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "IaX6IuZYQPDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_local = np.load('Data/Final_Data_Transformers/final_transformers_local_met_data_train.npz', allow_pickle=True)['arr_0']\n",
        "X_remote = np.load('Data/Final_Data_Transformers/final_transformers_station_metaq_data_train.npz', allow_pickle=True)['arr_0']\n",
        "y_train = np.load('Data/Final_Data_Transformers/transformers_local_aq_data_train.npz', allow_pickle=True)['arr_0']\n",
        "y_train = y_train.reshape((-1,))"
      ],
      "metadata": {
        "id": "2jiBg0WKH-42"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batched_train_data = batchify_data(X_local, X_remote, y_train)"
      ],
      "metadata": {
        "id": "81IxDElX7tXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = MyModel(\n",
        "    dim_model_local=list(X_local.shape[1:]), dim_model_remote=list(X_remote.shape[1:]), num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1\n",
        ").to(device)\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "mWZuxveFAZT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, opt, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    preds = []\n",
        "    for i in tqdm(range(len(batched_train_data))):\n",
        "        X_l, X_r, y = batched_train_data[i][0], batched_train_data[i][1], batched_train_data[i][2]\n",
        "        X_l, X_r, y = torch.tensor(X_l).to(device).float(), torch.tensor(X_r).to(device).float(), torch.tensor(y).to(device).float()\n",
        "\n",
        "        pred = model(X_l, X_r)\n",
        "        preds.append(pred.detach().numpy().reshape(-1))\n",
        "        loss = loss_fn(pred, y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    \n",
        "    preds = np.array(preds)\n",
        "    preds = preds.flatten()\n",
        "    total_loss = np.sqrt(mean_squared_error(preds, y_train))\n",
        "    \n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "NHmP_n0GB8ps"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, opt, loss_fn, epochs):\n",
        "    \"\"\"\n",
        "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
        "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
        "    \"\"\"\n",
        "    \n",
        "    # Used for plotting later on\n",
        "    train_loss_list = []\n",
        "    \n",
        "    print(\"Training model\")\n",
        "    for epoch in range(epochs):\n",
        "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
        "        \n",
        "        train_loss = train_loop(model, opt, loss_fn)\n",
        "        torch.save(model.state_dict(), f'./Model Checkpoints/model_{epoch}_epoch.pt')\n",
        "        train_loss_list += [train_loss]\n",
        "        \n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "        print()\n",
        "        \n",
        "    return train_loss_list\n",
        "    \n",
        "train_loss_list = fit(model, opt, loss_fn, 10)"
      ],
      "metadata": {
        "id": "hoKILnILB8nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting Train Loss"
      ],
      "metadata": {
        "id": "Syp_w07DQtBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(10)\n",
        "plt.plot(x, train_loss_list)\n",
        "plt.title(\"Train Loss vs Epoch\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Train Loss')\n",
        "plt.savefig('./FinalReportandppt/Plots/train_loss_vs_epochs.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "cSRyRvMQkans",
        "outputId": "ff8f9807-aea0-4a58-ba85-42b27463d767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "esBqmKCYlNYJ",
        "outputId": "d372a830-4993-4615-e4da-bf1955d9a04f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "31qeIbd6QyJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_local_test = np.load('Data/Final_Data_Transformers/final_transformers_local_met_data_test.npz', allow_pickle=True)['arr_0']\n",
        "X_remote_test = np.load('Data/Final_Data_Transformers/final_transformers_station_metaq_data_test.npz', allow_pickle=True)['arr_0']\n",
        "y_test = np.load('Data/Final_Data_Transformers/transformers_local_aq_data_test.npz', allow_pickle=True)['arr_0']\n",
        "y_test = y_test.reshape((-1,))"
      ],
      "metadata": {
        "id": "TuH3rALKROd9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_local_test.shape, X_remote_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34FZQq-pX5ST",
        "outputId": "39c5b904-1bbb-4787-e308-384288707067"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8628, 24, 30), (8628, 23, 24, 31), (8628,))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = MyModel(\n",
        "    dim_model_local=list(X_local.shape[1:]), dim_model_remote=list(X_remote.shape[1:]), num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1\n",
        ").to(device)\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.MSELoss()\n",
        "model.load_state_dict(torch.load('./Model Checkpoints/model_9_epoch.pt'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "zO5TEsB_Q0-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, X_local, X_remote):\n",
        "    model.eval()\n",
        "    X_l, X_r= torch.tensor(X_local).to(device).float(), torch.tensor(X_remote).to(device).float()\n",
        "    pred = model(X_l, X_r)\n",
        "    return pred.detach().item()"
      ],
      "metadata": {
        "id": "CA15CBd7TRJd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batched_test_data = batchify_data(X_local_test, X_remote_test, y_test, batch_size=12)"
      ],
      "metadata": {
        "id": "hg3u3weqSEU8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(model, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    preds = []\n",
        "    for i in tqdm(range(len(batched_test_data))):\n",
        "        X_l, X_r, y = batched_test_data[i][0], batched_test_data[i][1], batched_test_data[i][2]\n",
        "        X_l, X_r, y = torch.tensor(X_l).to(device).float(), torch.tensor(X_r).to(device).float(), torch.tensor(y).to(device).float()\n",
        "\n",
        "        pred = model(X_l, X_r)\n",
        "        preds.append(pred.detach().numpy().reshape(-1))\n",
        "    \n",
        "    preds = np.array(preds)\n",
        "    preds = preds.flatten()\n",
        "    total_loss = np.sqrt(mean_squared_error(preds, y_test))\n",
        "    \n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "gPJQld15R-U0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = test_loop(model, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7u1aov1R2iX",
        "outputId": "25b1515c-0c53-4b0e-a0a5-d3d200e05137"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 719/719 [00:51<00:00, 14.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss"
      ],
      "metadata": {
        "id": "09VopFMdWs3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a55ebf2c-ec6f-41e6-868e-304bf2b94fcb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.36726488614612"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}