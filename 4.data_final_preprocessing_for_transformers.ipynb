{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_rB6wze5Zi8t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "import tqdm\n",
        "from IPython.display import clear_output\n",
        "from multiprocessing import Pool\n",
        "from time import time\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePh_SspVNiRC",
        "outputId": "6237ccf8-c74e-405c-e385-3b0c06ac935b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6G8kndrTcO2",
        "outputId": "c4e7bb23-28b3-4e30-bb2f-c514a4ce766e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/Machine Learning - Transformers\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/Shareddrives/Machine Learning - Transformers\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "6bWRzwlVvPqw"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('Data/data_final_transformers_0_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('Data/data_final_transformers_0_test.csv')"
      ],
      "metadata": {
        "id": "WCZyzn_v_BL2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### make a time index to split into 24 hours"
      ],
      "metadata": {
        "id": "NdOtvI4_feEa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "aieZLpzrbc2y"
      },
      "outputs": [],
      "source": [
        "time_index = [1]\n",
        "\n",
        "for i in range(len(train_data['time'])-1):\n",
        "  \n",
        "  if train_data['time'][i] != train_data['time'][i+1]:\n",
        "    time_index.append(time_index[-1]+1) \n",
        "  else:\n",
        "    time_index.append(time_index[-1]) \n",
        "train_data.insert(len(train_data.columns), \"time_index\", time_index)\n",
        "train_stations = train_data.station_id.unique()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_index = [1]\n",
        "\n",
        "for i in range(len(test_data['time'])-1):\n",
        "  \n",
        "  if test_data['time'][i] != test_data['time'][i+1]:\n",
        "    time_index.append(time_index[-1]+1) \n",
        "  else:\n",
        "    time_index.append(time_index[-1]) \n",
        "test_data.insert(len(test_data.columns), \"time_index\", time_index)\n",
        "test_stations = test_data.station_id.unique()\n",
        "  "
      ],
      "metadata": {
        "id": "XxvVqwNOK7yx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data into local and remote stations and split into 24 hours"
      ],
      "metadata": {
        "id": "Wy8PIURvfjYK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5yxPthDwZzO5"
      },
      "outputs": [],
      "source": [
        "time_window = 24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "I_9q-EzzaKTH"
      },
      "outputs": [],
      "source": [
        "def get_train_features(i):\n",
        "    # For train data\n",
        "    remote_metaq_data = []\n",
        "    remote_dist_data = []\n",
        "    local_met_data = []\n",
        "    local_aq_data = []\n",
        "    local_stations = []\n",
        "    \n",
        "    tmp_df = train_data[(train_data['time_index']>=i) & (train_data['time_index']<i+time_window)]\n",
        "\n",
        "    \n",
        "    for station in train_stations:\n",
        "        # Remote side\n",
        "        remote_side = tmp_df[tmp_df.station_id != station]\n",
        "        remote_side['PM25_Concentration'] = MinMaxScaler().fit_transform(np.array(remote_side['PM25_Concentration']).reshape(-1,1))\n",
        "        station_met_aq = remote_side.drop(columns=['station_id', 'longitude', 'latitude'])\n",
        "        station_met_aq2 = np.array(np.split(station_met_aq.values, time_window, axis=0)).swapaxes(0,1).swapaxes(1,2)[np.newaxis, :]\n",
        "        remote_metaq_data.append(station_met_aq2)\n",
        "        \n",
        "        # Local side\n",
        "        local_side = tmp_df[tmp_df.station_id == station]\n",
        "        local_stations.append(local_side['station_id'].values[-1].reshape(-1,1))\n",
        "        local_met = local_side.drop(columns=['station_id', 'longitude', 'latitude', 'PM25_Concentration']).values.swapaxes(0,1)[np.newaxis, :]\n",
        "        local_met_data.append(local_met)\n",
        "        local_aq = local_side['PM25_Concentration'].values[-1].reshape(-1,1)\n",
        "        local_aq_data.append(local_aq)\n",
        "        \n",
        "        station_dist = (remote_side.drop_duplicates('station_id')[['longitude', 'latitude']].values -\\\n",
        "        local_side.drop_duplicates('station_id')[['longitude', 'latitude']].values)[np.newaxis, :]\n",
        "        remote_dist_data.append(station_dist)\n",
        "    return [np.concatenate(remote_metaq_data), \n",
        "            np.concatenate(remote_dist_data), \n",
        "            np.concatenate(local_met_data), \n",
        "            np.concatenate(local_aq_data),\n",
        "            np.concatenate(local_stations)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "3Kv-p3LsejqX"
      },
      "outputs": [],
      "source": [
        "def get_test_features(i):\n",
        "    # For test data\n",
        "    remote_metaq_data = []\n",
        "    remote_dist_data = []\n",
        "    local_met_data = []\n",
        "    local_aq_data = []\n",
        "    local_stations = []\n",
        "    \n",
        "    tmp_df_tst = test_data[(test_data['time_index']>=i) & (test_data['time_index']<i+time_window)]\n",
        "    \n",
        "    remote_side = train_data[(train_data['time_index']>=i) & (train_data['time_index']<i+time_window)]\n",
        "    remote_side['PM25_Concentration'] = MinMaxScaler().fit_transform(np.array(remote_side['PM25_Concentration']).reshape(-1,1))\n",
        "    remote_met_aq = remote_side.drop(columns=['station_id', 'longitude', 'latitude'])\n",
        "    remote_met_aq2 = np.array(np.split(remote_met_aq.values, time_window, axis=0)).swapaxes(0,1).swapaxes(1,2)[np.newaxis, :]\n",
        "    \n",
        "    for station in test_stations:\n",
        "        remote_metaq_data.append(remote_met_aq2)\n",
        "        \n",
        "        # Local side\n",
        "        local_side = tmp_df_tst[tmp_df_tst.station_id == station]\n",
        "        local_stations.append(local_side['station_id'].values[-1].reshape(-1,1))\n",
        "        local_met = local_side.drop(columns=['station_id', 'longitude', 'latitude', \n",
        "                                             'PM25_Concentration']).values.swapaxes(0,1)[np.newaxis, :]\n",
        "        local_met_data.append(local_met)\n",
        "        local_aq = local_side['PM25_Concentration'].values[-1].reshape(-1,1)\n",
        "        local_aq_data.append(local_aq)\n",
        "        \n",
        "        remote_dist = (remote_side.drop_duplicates('station_id')[['longitude', 'latitude']].values -\\\n",
        "        local_side.drop_duplicates('station_id')[['longitude', 'latitude']].values)[np.newaxis, :]\n",
        "        remote_dist_data.append(remote_dist)\n",
        "    \n",
        "    return [np.concatenate(remote_metaq_data), \n",
        "            np.concatenate(remote_dist_data), \n",
        "            np.concatenate(local_met_data), \n",
        "            np.concatenate(local_aq_data),\n",
        "            np.concatenate(local_stations)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rXZZx-qE4z_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c8b9c3-d70b-44cf-dba1-ae8b6793e630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0\n",
            "test start\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 719/721 [00:44<00:00, 16.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test finished\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "fold=0 # not using for loop to avoid ram overflow\n",
        "print('fold', fold)\n",
        "\n",
        "cols_to_scale = ['temperature','latitude','longitude','wind_speed','humidity', 'pressure']\n",
        "scaler =  MinMaxScaler().fit(train_data[cols_to_scale])\n",
        "train_data[cols_to_scale] = scaler.transform(train_data[cols_to_scale])\n",
        "scaler =  MinMaxScaler().fit(test_data[cols_to_scale])\n",
        "test_data[cols_to_scale] = scaler.transform(test_data[cols_to_scale])\n",
        "                                         \n",
        "# print('train start')\n",
        "# time_range = train_data.time_index.unique()\n",
        "# with Pool(26) as p:\n",
        "#     train_combo = list(tqdm.tqdm(p.imap(get_train_features, range(1, len(time_range)-24)), total=len(time_range)-24+1))\n",
        "# print('train finished')\n",
        "print('test start')\n",
        "time_range = test_data.time_index.unique()\n",
        "with Pool(26) as p:\n",
        "    test_combo = list(tqdm.tqdm(p.imap(get_test_features, range(1, len(time_range)-24)), total=len(time_range)-24+1))\n",
        "print('test finished')\n",
        "\n",
        "# for combo_data, name in zip([train_combo], ['train']):\n",
        "#     station_metaq_data = np.concatenate([combo[0] for combo in combo_data])\n",
        "#     station_dist_data = np.concatenate([combo[1] for combo in combo_data])\n",
        "#     local_met_data = np.concatenate([combo[2] for combo in combo_data])\n",
        "#     local_aq_data = np.concatenate([combo[3] for combo in combo_data])\n",
        "#     local_stations = np.concatenate([combo[4] for combo in combo_data])\n",
        "#     np.savez('./Data/Final_Data_Transformers/transformers_station_metaq_data_'+name, station_metaq_data) \n",
        "#     np.savez('./Data/Final_Data_Transformers/transformers_station_dist_data_'+name, station_dist_data)\n",
        "#     np.savez('./Data/Final_Data_Transformers/transformers_local_met_data_'+name, local_met_data)\n",
        "#     np.savez('./Data/Final_Data_Transformers/transformers_local_aq_data_'+name, local_aq_data)\n",
        "#     np.savez('./Data/Final_Data_Transformers/transformers_local_stationids_'+name, local_stations)\n",
        "\n",
        "for combo_data, name in zip([test_combo], ['test']):\n",
        "    station_metaq_data = np.concatenate([combo[0] for combo in combo_data])\n",
        "    station_dist_data = np.concatenate([combo[1] for combo in combo_data])\n",
        "    local_met_data = np.concatenate([combo[2] for combo in combo_data])\n",
        "    local_aq_data = np.concatenate([combo[3] for combo in combo_data])\n",
        "    local_stations = np.concatenate([combo[4] for combo in combo_data])\n",
        "    np.savez('./Data/Final_Data_Transformers/transformers_station_metaq_data_'+name, station_metaq_data) \n",
        "    np.savez('./Data/Final_Data_Transformers/transformers_station_dist_data_'+name, station_dist_data)\n",
        "    np.savez('./Data/Final_Data_Transformers/transformers_local_met_data_'+name, local_met_data)\n",
        "    np.savez('./Data/Final_Data_Transformers/transformers_local_aq_data_'+name, local_aq_data)\n",
        "    np.savez('./Data/Final_Data_Transformers/transformers_local_stationids_'+name, local_stations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjH-1I-Y2E_L",
        "outputId": "bf7a4453-ff3c-413b-c61d-1ab150991a0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8628, 32, 24), (8628, 1), (8628, 23, 33, 24), (8628, 23, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "local_met_data.shape, local_aq_data.shape, station_metaq_data.shape, station_dist_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# name = 'train'\n",
        "# local_met_data = np.load(f'Data/Final_Data_Transformers/transformers_local_met_data_{name}.npz', allow_pickle=True)['arr_0']\n",
        "# local_met = np.array([np.delete(local_met_data[i].T, [0,31], axis=1) for i in range(local_met_data.shape[0])], dtype=float)\n",
        "# np.savez('./Data/Final_Data_Transformers/final_transformers_local_met_data_'+ name, local_met)"
      ],
      "metadata": {
        "id": "tsBpGbfdO-I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'test'\n",
        "local_met_data = np.load(f'Data/Final_Data_Transformers/transformers_local_met_data_{name}.npz', allow_pickle=True)['arr_0']\n",
        "local_met = np.array([np.delete(local_met_data[i].T, [0,31], axis=1) for i in range(local_met_data.shape[0])], dtype=float)\n",
        "np.savez('./Data/Final_Data_Transformers/final_transformers_local_met_data_'+ name, local_met)"
      ],
      "metadata": {
        "id": "2EgtFYx4hPNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(local_met[0])"
      ],
      "metadata": {
        "id": "WcShfp78PsCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgTKEHOz9ngV"
      },
      "outputs": [],
      "source": [
        "# name = 'train'\n",
        "# station_metaq_data = np.load(f'Data/Final_Data_Transformers/transformers_station_metaq_data_{name}.npz', allow_pickle=True)['arr_0']\n",
        "# station_metaq = []\n",
        "# for i in range(station_metaq_data.shape[0]):\n",
        "#   tmp = np.array([np.delete(station_metaq_data[i][j].T, [0,32], axis=1) for j in range(station_metaq_data.shape[1])], dtype=float)\n",
        "#   station_metaq.append(tmp)\n",
        "# station_metaq = np.array(station_metaq)\n",
        "# np.savez('./Data/Final_Data_Transformers/final_transformers_station_metaq_data_'+ name, station_metaq)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'test'\n",
        "station_metaq_data = np.load(f'Data/Final_Data_Transformers/transformers_station_metaq_data_{name}.npz', allow_pickle=True)['arr_0']\n",
        "station_metaq = []\n",
        "for i in range(station_metaq_data.shape[0]):\n",
        "  tmp = np.array([np.delete(station_metaq_data[i][j].T, [0,32], axis=1) for j in range(station_metaq_data.shape[1])], dtype=float)\n",
        "  station_metaq.append(tmp)\n",
        "station_metaq = np.array(station_metaq)\n",
        "np.savez('./Data/Final_Data_Transformers/final_transformers_station_metaq_data_'+ name, station_metaq)"
      ],
      "metadata": {
        "id": "V1dxO2G8hp5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok0JaSRb24Y5"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(station_metaq[0][0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data final preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}